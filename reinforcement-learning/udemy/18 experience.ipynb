{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "\n",
    "import math, random, time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "\n",
    "learning_rate = 0.02\n",
    "num_episodes = 500\n",
    "gamma = 1\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "replay_mem_size = 50000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    return egreedy_final + (egreedy - egreedy_final) * math.exp(-1 * steps_done / egreedy_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    " \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory[self.position] = transition\n",
    "        \n",
    "        self.position = ( self.position + 1 ) % self.capacity\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, number_of_outputs)\n",
    "        self.activation = nn.Tanh()\n",
    "#         self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        return self.linear2(output1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetAgent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork()\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def select_action(self, state, epsion):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = torch.Tensor(state)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn, 0)[1]\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = torch.Tensor(state) #.to(device)\n",
    "        new_state = torch.Tensor(new_state) #.to(device)\n",
    "        reward = torch.Tensor(reward) #.to(device)\n",
    "        action = torch.LongTensor(action) #.to(device)\n",
    "        done = torch.Tensor(done) #.to(device)\n",
    "\n",
    "        new_state_values = self.nn(new_state).detach()\n",
    "        max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "        target_value = reward + ( 1 - done ) * gamma * max_new_state_values\n",
    "  \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 0 ***                       \n",
      "Av.reward: [last 10]: 1.20, [last 100]: 0.12, [all]: 12.00                       \n",
      "epsilon: 0.88, frames_total: 12\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 10 ***                       \n",
      "Av.reward: [last 10]: 37.20, [last 100]: 3.84, [all]: 34.91                       \n",
      "epsilon: 0.42, frames_total: 384\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 20 ***                       \n",
      "Av.reward: [last 10]: 175.70, [last 100]: 21.41, [all]: 101.95                       \n",
      "epsilon: 0.01, frames_total: 2141\n",
      "Elapsed time:  00:00:01\n",
      "\n",
      "*** Episode 30 ***                       \n",
      "Av.reward: [last 10]: 187.30, [last 100]: 40.14, [all]: 129.48                       \n",
      "epsilon: 0.00, frames_total: 4014\n",
      "Elapsed time:  00:00:03\n",
      "\n",
      "*** Episode 40 ***                       \n",
      "Av.reward: [last 10]: 192.40, [last 100]: 59.38, [all]: 144.83                       \n",
      "epsilon: 0.00, frames_total: 5938\n",
      "Elapsed time:  00:00:05\n",
      "\n",
      "*** Episode 50 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 79.38, [all]: 155.65                       \n",
      "epsilon: 0.00, frames_total: 7938\n",
      "Elapsed time:  00:00:06\n",
      "\n",
      "*** Episode 60 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 99.38, [all]: 162.92                       \n",
      "epsilon: 0.00, frames_total: 9938\n",
      "Elapsed time:  00:00:08\n",
      "\n",
      "*** Episode 70 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 119.38, [all]: 168.14                       \n",
      "epsilon: 0.00, frames_total: 11938\n",
      "Elapsed time:  00:00:10\n",
      "\n",
      "*** Episode 80 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 139.38, [all]: 172.07                       \n",
      "epsilon: 0.00, frames_total: 13938\n",
      "Elapsed time:  00:00:11\n",
      "\n",
      "*** Episode 90 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 159.38, [all]: 175.14                       \n",
      "epsilon: 0.00, frames_total: 15938\n",
      "Elapsed time:  00:00:13\n",
      "\n",
      "*** Episode 100 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 179.26, [all]: 177.60                       \n",
      "epsilon: 0.00, frames_total: 17938\n",
      "Elapsed time:  00:00:15\n",
      "SOLVED! After 110 episodes \n",
      "\n",
      "*** Episode 110 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 195.54, [all]: 179.62                       \n",
      "epsilon: 0.00, frames_total: 19938\n",
      "Elapsed time:  00:00:17\n",
      "\n",
      "*** Episode 120 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 197.97, [all]: 181.31                       \n",
      "epsilon: 0.00, frames_total: 21938\n",
      "Elapsed time:  00:00:18\n",
      "\n",
      "*** Episode 130 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.24, [all]: 182.73                       \n",
      "epsilon: 0.00, frames_total: 23938\n",
      "Elapsed time:  00:00:20\n",
      "\n",
      "*** Episode 140 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 183.96                       \n",
      "epsilon: 0.00, frames_total: 25938\n",
      "Elapsed time:  00:00:22\n",
      "\n",
      "*** Episode 150 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 185.02                       \n",
      "epsilon: 0.00, frames_total: 27938\n",
      "Elapsed time:  00:00:24\n",
      "\n",
      "*** Episode 160 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 185.95                       \n",
      "epsilon: 0.00, frames_total: 29938\n",
      "Elapsed time:  00:00:25\n",
      "\n",
      "*** Episode 170 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 186.77                       \n",
      "epsilon: 0.00, frames_total: 31938\n",
      "Elapsed time:  00:00:27\n",
      "\n",
      "*** Episode 180 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 187.50                       \n",
      "epsilon: 0.00, frames_total: 33938\n",
      "Elapsed time:  00:00:29\n",
      "\n",
      "*** Episode 190 ***                       \n",
      "Av.reward: [last 10]: 199.90, [last 100]: 199.99, [all]: 188.15                       \n",
      "epsilon: 0.00, frames_total: 35937\n",
      "Elapsed time:  00:00:31\n",
      "\n",
      "*** Episode 200 ***                       \n",
      "Av.reward: [last 10]: 199.90, [last 100]: 199.98, [all]: 188.74                       \n",
      "epsilon: 0.00, frames_total: 37936\n",
      "Elapsed time:  00:00:32\n",
      "\n",
      "*** Episode 210 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.98, [all]: 189.27                       \n",
      "epsilon: 0.00, frames_total: 39936\n",
      "Elapsed time:  00:00:34\n",
      "\n",
      "*** Episode 220 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.98, [all]: 189.76                       \n",
      "epsilon: 0.00, frames_total: 41936\n",
      "Elapsed time:  00:00:36\n",
      "\n",
      "*** Episode 230 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.98, [all]: 190.20                       \n",
      "epsilon: 0.00, frames_total: 43936\n",
      "Elapsed time:  00:00:38\n",
      "\n",
      "*** Episode 240 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.98, [all]: 190.61                       \n",
      "epsilon: 0.00, frames_total: 45936\n",
      "Elapsed time:  00:00:40\n",
      "\n",
      "*** Episode 250 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.98, [all]: 190.98                       \n",
      "epsilon: 0.00, frames_total: 47936\n",
      "Elapsed time:  00:00:41\n",
      "\n",
      "*** Episode 260 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.98, [all]: 191.33                       \n",
      "epsilon: 0.00, frames_total: 49936\n",
      "Elapsed time:  00:00:43\n",
      "\n",
      "*** Episode 270 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.98, [all]: 191.65                       \n",
      "epsilon: 0.00, frames_total: 51936\n",
      "Elapsed time:  00:00:45\n",
      "\n",
      "*** Episode 280 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.98, [all]: 191.94                       \n",
      "epsilon: 0.00, frames_total: 53936\n",
      "Elapsed time:  00:00:47\n",
      "\n",
      "*** Episode 290 ***                       \n",
      "Av.reward: [last 10]: 171.90, [last 100]: 197.18, [all]: 191.25                       \n",
      "epsilon: 0.00, frames_total: 55655\n",
      "Elapsed time:  00:00:48\n",
      "\n",
      "*** Episode 300 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 197.19, [all]: 191.54                       \n",
      "epsilon: 0.00, frames_total: 57655\n",
      "Elapsed time:  00:00:50\n",
      "\n",
      "*** Episode 310 ***                       \n",
      "Av.reward: [last 10]: 169.00, [last 100]: 194.09, [all]: 190.82                       \n",
      "epsilon: 0.00, frames_total: 59345\n",
      "Elapsed time:  00:00:52\n",
      "\n",
      "*** Episode 320 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 194.09, [all]: 191.11                       \n",
      "epsilon: 0.00, frames_total: 61345\n",
      "Elapsed time:  00:00:54\n",
      "\n",
      "*** Episode 330 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 194.09, [all]: 191.37                       \n",
      "epsilon: 0.00, frames_total: 63345\n",
      "Elapsed time:  00:00:56\n",
      "\n",
      "*** Episode 340 ***                       \n",
      "Av.reward: [last 10]: 183.10, [last 100]: 192.40, [all]: 191.13                       \n",
      "epsilon: 0.00, frames_total: 65176\n",
      "Elapsed time:  00:00:57\n",
      "\n",
      "*** Episode 350 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 192.40, [all]: 191.38                       \n",
      "epsilon: 0.00, frames_total: 67176\n",
      "Elapsed time:  00:00:59\n",
      "\n",
      "*** Episode 360 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 192.40, [all]: 191.62                       \n",
      "epsilon: 0.00, frames_total: 69176\n",
      "Elapsed time:  00:01:01\n",
      "\n",
      "*** Episode 370 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 192.40, [all]: 191.85                       \n",
      "epsilon: 0.00, frames_total: 71176\n",
      "Elapsed time:  00:01:03\n",
      "\n",
      "*** Episode 380 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 192.40, [all]: 192.06                       \n",
      "epsilon: 0.00, frames_total: 73176\n",
      "Elapsed time:  00:01:04\n",
      "\n",
      "*** Episode 390 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 195.21, [all]: 192.27                       \n",
      "epsilon: 0.00, frames_total: 75176\n",
      "Elapsed time:  00:01:06\n",
      "\n",
      "*** Episode 400 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 195.21, [all]: 192.46                       \n",
      "epsilon: 0.00, frames_total: 77176\n",
      "Elapsed time:  00:01:08\n",
      "\n",
      "*** Episode 410 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 198.31, [all]: 192.64                       \n",
      "epsilon: 0.00, frames_total: 79176\n",
      "Elapsed time:  00:01:10\n",
      "\n",
      "*** Episode 420 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 198.31, [all]: 192.82                       \n",
      "epsilon: 0.00, frames_total: 81176\n",
      "Elapsed time:  00:01:12\n",
      "\n",
      "*** Episode 430 ***                       \n",
      "Av.reward: [last 10]: 197.90, [last 100]: 198.10, [all]: 192.94                       \n",
      "epsilon: 0.00, frames_total: 83155\n",
      "Elapsed time:  00:01:13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 440 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.79, [all]: 193.10                       \n",
      "epsilon: 0.00, frames_total: 85155\n",
      "Elapsed time:  00:01:15\n",
      "\n",
      "*** Episode 450 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.79, [all]: 193.25                       \n",
      "epsilon: 0.00, frames_total: 87155\n",
      "Elapsed time:  00:01:17\n",
      "\n",
      "*** Episode 460 ***                       \n",
      "Av.reward: [last 10]: 196.80, [last 100]: 199.47, [all]: 193.33                       \n",
      "epsilon: 0.00, frames_total: 89123\n",
      "Elapsed time:  00:01:19\n",
      "\n",
      "*** Episode 470 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.47, [all]: 193.47                       \n",
      "epsilon: 0.00, frames_total: 91123\n",
      "Elapsed time:  00:01:20\n",
      "\n",
      "*** Episode 480 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.47, [all]: 193.60                       \n",
      "epsilon: 0.00, frames_total: 93123\n",
      "Elapsed time:  00:01:22\n",
      "\n",
      "*** Episode 490 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.47, [all]: 193.73                       \n",
      "epsilon: 0.00, frames_total: 95123\n",
      "Elapsed time:  00:01:24\n",
      "Average reward: 193.85\n",
      "Average number of steps (last 100 episodes): 199.47\n",
      "Solved after 110 episodes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119bf0cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qnet_agent = QNetAgent()\n",
    "memory = ExperienceReplay(replay_mem_size)\n",
    "\n",
    "steps_total = []\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    step = 0\n",
    "    while True:\n",
    "        \n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        \n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        qnet_agent.optimize()\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            \n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            \n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "            \n",
    "            if (i_episode % report_interval == 0):\n",
    "                \n",
    "                \n",
    "                \n",
    "                print(\"\\n*** Episode %i *** \\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f \\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\" \n",
    "                  % \n",
    "                  ( i_episode,\n",
    "                    report_interval,\n",
    "                    sum(steps_total[-report_interval:])/report_interval,\n",
    "                    mean_reward_100,\n",
    "                    sum(steps_total)/len(steps_total),\n",
    "                    epsilon,\n",
    "                    frames_total\n",
    "                          ) \n",
    "                  )\n",
    "                  \n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time: \", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "print(\"Average reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "      print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color='red')\n",
    "plt.show()\n",
    "      \n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
