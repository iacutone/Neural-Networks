{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "\n",
    "import math, random, time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_episodes = 500\n",
    "gamma = 1\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "replay_mem_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "update_target_frequency = 100\n",
    "clip_error = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    return egreedy_final + (egreedy - egreedy_final) * math.exp(-1 * steps_done / egreedy_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    " \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory[self.position] = transition\n",
    "        \n",
    "        self.position = ( self.position + 1 ) % self.capacity\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, number_of_outputs)\n",
    "        self.activation = nn.Tanh()\n",
    "#         self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        return self.linear2(output1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetAgent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork()\n",
    "        self.target_nn = NeuralNetwork()\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        self.update_target_counter = 0\n",
    "        \n",
    "    def select_action(self, state, epsion):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = torch.Tensor(state)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn, 0)[1]\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = torch.Tensor(state) #.to(device)\n",
    "        new_state = torch.Tensor(new_state) #.to(device)\n",
    "        reward = torch.Tensor(reward) #.to(device)\n",
    "        action = torch.LongTensor(action) #.to(device)\n",
    "        done = torch.Tensor(done) #.to(device)\n",
    "\n",
    "        new_state_values = self.target_nn(new_state).detach()\n",
    "        max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "        target_value = reward + ( 1 - done ) * gamma * max_new_state_values\n",
    "  \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1,1)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        self.update_target_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 0 ***                       \n",
      "Av.reward: [last 10]: 1.20, [last 100]: 0.12, [all]: 12.00                       \n",
      "epsilon: 0.88, frames_total: 12\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 10 ***                       \n",
      "Av.reward: [last 10]: 27.40, [last 100]: 2.86, [all]: 26.00                       \n",
      "epsilon: 0.51, frames_total: 286\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 20 ***                       \n",
      "Av.reward: [last 10]: 122.90, [last 100]: 15.15, [all]: 72.14                       \n",
      "epsilon: 0.04, frames_total: 1515\n",
      "Elapsed time:  00:00:01\n",
      "\n",
      "*** Episode 30 ***                       \n",
      "Av.reward: [last 10]: 196.60, [last 100]: 34.81, [all]: 112.29                       \n",
      "epsilon: 0.00, frames_total: 3481\n",
      "Elapsed time:  00:00:03\n",
      "\n",
      "*** Episode 40 ***                       \n",
      "Av.reward: [last 10]: 199.60, [last 100]: 54.77, [all]: 133.59                       \n",
      "epsilon: 0.00, frames_total: 5477\n",
      "Elapsed time:  00:00:04\n",
      "\n",
      "*** Episode 50 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 74.77, [all]: 146.61                       \n",
      "epsilon: 0.00, frames_total: 7477\n",
      "Elapsed time:  00:00:06\n",
      "\n",
      "*** Episode 60 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 94.77, [all]: 155.36                       \n",
      "epsilon: 0.00, frames_total: 9477\n",
      "Elapsed time:  00:00:08\n",
      "\n",
      "*** Episode 70 ***                       \n",
      "Av.reward: [last 10]: 199.10, [last 100]: 114.68, [all]: 161.52                       \n",
      "epsilon: 0.00, frames_total: 11468\n",
      "Elapsed time:  00:00:10\n",
      "\n",
      "*** Episode 80 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 134.68, [all]: 166.27                       \n",
      "epsilon: 0.00, frames_total: 13468\n",
      "Elapsed time:  00:00:12\n",
      "\n",
      "*** Episode 90 ***                       \n",
      "Av.reward: [last 10]: 197.10, [last 100]: 154.39, [all]: 169.66                       \n",
      "epsilon: 0.00, frames_total: 15439\n",
      "Elapsed time:  00:00:13\n",
      "\n",
      "*** Episode 100 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 174.27, [all]: 172.66                       \n",
      "epsilon: 0.00, frames_total: 17439\n",
      "Elapsed time:  00:00:15\n",
      "\n",
      "*** Episode 110 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 191.53, [all]: 175.13                       \n",
      "epsilon: 0.00, frames_total: 19439\n",
      "Elapsed time:  00:00:17\n",
      "SOLVED! After 112 episodes \n",
      "\n",
      "*** Episode 120 ***                       \n",
      "Av.reward: [last 10]: 195.00, [last 100]: 198.74, [all]: 176.77                       \n",
      "epsilon: 0.00, frames_total: 21389\n",
      "Elapsed time:  00:00:18\n",
      "\n",
      "*** Episode 130 ***                       \n",
      "Av.reward: [last 10]: 196.90, [last 100]: 198.77, [all]: 178.31                       \n",
      "epsilon: 0.00, frames_total: 23358\n",
      "Elapsed time:  00:00:20\n",
      "\n",
      "*** Episode 140 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 198.81, [all]: 179.84                       \n",
      "epsilon: 0.00, frames_total: 25358\n",
      "Elapsed time:  00:00:22\n",
      "\n",
      "*** Episode 150 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 198.81, [all]: 181.18                       \n",
      "epsilon: 0.00, frames_total: 27358\n",
      "Elapsed time:  00:00:24\n",
      "\n",
      "*** Episode 160 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 198.81, [all]: 182.35                       \n",
      "epsilon: 0.00, frames_total: 29358\n",
      "Elapsed time:  00:00:26\n",
      "\n",
      "*** Episode 170 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 198.90, [all]: 183.38                       \n",
      "epsilon: 0.00, frames_total: 31358\n",
      "Elapsed time:  00:00:27\n",
      "\n",
      "*** Episode 180 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 198.90, [all]: 184.30                       \n",
      "epsilon: 0.00, frames_total: 33358\n",
      "Elapsed time:  00:00:29\n",
      "\n",
      "*** Episode 190 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.19, [all]: 185.12                       \n",
      "epsilon: 0.00, frames_total: 35358\n",
      "Elapsed time:  00:00:31\n",
      "\n",
      "*** Episode 200 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.19, [all]: 185.86                       \n",
      "epsilon: 0.00, frames_total: 37358\n",
      "Elapsed time:  00:00:33\n",
      "\n",
      "*** Episode 210 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.19, [all]: 186.53                       \n",
      "epsilon: 0.00, frames_total: 39358\n",
      "Elapsed time:  00:00:35\n",
      "\n",
      "*** Episode 220 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.69, [all]: 187.14                       \n",
      "epsilon: 0.00, frames_total: 41358\n",
      "Elapsed time:  00:00:36\n",
      "\n",
      "*** Episode 230 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 187.70                       \n",
      "epsilon: 0.00, frames_total: 43358\n",
      "Elapsed time:  00:00:38\n",
      "\n",
      "*** Episode 240 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 188.21                       \n",
      "epsilon: 0.00, frames_total: 45358\n",
      "Elapsed time:  00:00:40\n",
      "\n",
      "*** Episode 250 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 188.68                       \n",
      "epsilon: 0.00, frames_total: 47358\n",
      "Elapsed time:  00:00:42\n",
      "\n",
      "*** Episode 260 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 189.11                       \n",
      "epsilon: 0.00, frames_total: 49358\n",
      "Elapsed time:  00:00:44\n",
      "\n",
      "*** Episode 270 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 189.51                       \n",
      "epsilon: 0.00, frames_total: 51358\n",
      "Elapsed time:  00:00:45\n",
      "\n",
      "*** Episode 280 ***                       \n",
      "Av.reward: [last 10]: 196.70, [last 100]: 199.67, [all]: 189.77                       \n",
      "epsilon: 0.00, frames_total: 53325\n",
      "Elapsed time:  00:00:47\n",
      "\n",
      "*** Episode 290 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.67, [all]: 190.12                       \n",
      "epsilon: 0.00, frames_total: 55325\n",
      "Elapsed time:  00:00:49\n",
      "\n",
      "*** Episode 300 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.67, [all]: 190.45                       \n",
      "epsilon: 0.00, frames_total: 57325\n",
      "Elapsed time:  00:00:51\n",
      "\n",
      "*** Episode 310 ***                       \n",
      "Av.reward: [last 10]: 167.80, [last 100]: 196.45, [all]: 189.72                       \n",
      "epsilon: 0.00, frames_total: 59003\n",
      "Elapsed time:  00:00:52\n",
      "\n",
      "*** Episode 320 ***                       \n",
      "Av.reward: [last 10]: 190.40, [last 100]: 195.49, [all]: 189.74                       \n",
      "epsilon: 0.00, frames_total: 60907\n",
      "Elapsed time:  00:00:54\n",
      "\n",
      "*** Episode 330 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 195.49, [all]: 190.05                       \n",
      "epsilon: 0.00, frames_total: 62907\n",
      "Elapsed time:  00:00:56\n",
      "\n",
      "*** Episode 340 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 195.49, [all]: 190.34                       \n",
      "epsilon: 0.00, frames_total: 64907\n",
      "Elapsed time:  00:00:58\n",
      "\n",
      "*** Episode 350 ***                       \n",
      "Av.reward: [last 10]: 197.50, [last 100]: 195.24, [all]: 190.55                       \n",
      "epsilon: 0.00, frames_total: 66882\n",
      "Elapsed time:  00:00:59\n",
      "\n",
      "*** Episode 360 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 195.24, [all]: 190.81                       \n",
      "epsilon: 0.00, frames_total: 68882\n",
      "Elapsed time:  00:01:01\n",
      "\n",
      "*** Episode 370 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 195.24, [all]: 191.06                       \n",
      "epsilon: 0.00, frames_total: 70882\n",
      "Elapsed time:  00:01:03\n",
      "\n",
      "*** Episode 380 ***                       \n",
      "Av.reward: [last 10]: 192.30, [last 100]: 194.80, [all]: 191.09                       \n",
      "epsilon: 0.00, frames_total: 72805\n",
      "Elapsed time:  00:01:05\n",
      "\n",
      "*** Episode 390 ***                       \n",
      "Av.reward: [last 10]: 193.30, [last 100]: 194.13, [all]: 191.15                       \n",
      "epsilon: 0.00, frames_total: 74738\n",
      "Elapsed time:  00:01:06\n",
      "\n",
      "*** Episode 400 ***                       \n",
      "Av.reward: [last 10]: 199.10, [last 100]: 194.04, [all]: 191.34                       \n",
      "epsilon: 0.00, frames_total: 76729\n",
      "Elapsed time:  00:01:08\n",
      "\n",
      "*** Episode 410 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 197.26, [all]: 191.55                       \n",
      "epsilon: 0.00, frames_total: 78729\n",
      "Elapsed time:  00:01:10\n",
      "\n",
      "*** Episode 420 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 198.22, [all]: 191.76                       \n",
      "epsilon: 0.00, frames_total: 80729\n",
      "Elapsed time:  00:01:12\n",
      "\n",
      "*** Episode 430 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 198.22, [all]: 191.95                       \n",
      "epsilon: 0.00, frames_total: 82729\n",
      "Elapsed time:  00:01:13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 440 ***                       \n",
      "Av.reward: [last 10]: 187.00, [last 100]: 196.92, [all]: 191.83                       \n",
      "epsilon: 0.00, frames_total: 84599\n",
      "Elapsed time:  00:01:15\n",
      "\n",
      "*** Episode 450 ***                       \n",
      "Av.reward: [last 10]: 188.70, [last 100]: 196.04, [all]: 191.76                       \n",
      "epsilon: 0.00, frames_total: 86486\n",
      "Elapsed time:  00:01:17\n",
      "\n",
      "*** Episode 460 ***                       \n",
      "Av.reward: [last 10]: 188.30, [last 100]: 194.87, [all]: 191.69                       \n",
      "epsilon: 0.00, frames_total: 88369\n",
      "Elapsed time:  00:01:19\n",
      "\n",
      "*** Episode 470 ***                       \n",
      "Av.reward: [last 10]: 190.90, [last 100]: 193.96, [all]: 191.67                       \n",
      "epsilon: 0.00, frames_total: 90278\n",
      "Elapsed time:  00:01:20\n",
      "\n",
      "*** Episode 480 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 194.73, [all]: 191.85                       \n",
      "epsilon: 0.00, frames_total: 92278\n",
      "Elapsed time:  00:01:22\n",
      "\n",
      "*** Episode 490 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 195.40, [all]: 192.01                       \n",
      "epsilon: 0.00, frames_total: 94278\n",
      "Elapsed time:  00:01:24\n",
      "Average reward: 192.16\n",
      "Average number of steps (last 100 episodes): 195.49\n",
      "Solved after 112 episodes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d2e7dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qnet_agent = QNetAgent()\n",
    "memory = ExperienceReplay(replay_mem_size)\n",
    "\n",
    "steps_total = []\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    step = 0\n",
    "    while True:\n",
    "        \n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        \n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        qnet_agent.optimize()\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            \n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            \n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "            \n",
    "            if (i_episode % report_interval == 0):\n",
    "                \n",
    "                \n",
    "                \n",
    "                print(\"\\n*** Episode %i *** \\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f \\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\" \n",
    "                  % \n",
    "                  ( i_episode,\n",
    "                    report_interval,\n",
    "                    sum(steps_total[-report_interval:])/report_interval,\n",
    "                    mean_reward_100,\n",
    "                    sum(steps_total)/len(steps_total),\n",
    "                    epsilon,\n",
    "                    frames_total\n",
    "                          ) \n",
    "                  )\n",
    "                  \n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time: \", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "print(\"Average reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "      print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color='red')\n",
    "plt.show()\n",
    "      \n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
