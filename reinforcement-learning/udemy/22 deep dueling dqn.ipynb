{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "\n",
    "import math, random, time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "\n",
    "learning_rate = 0.002\n",
    "num_episodes = 500\n",
    "gamma = 0.99\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "replay_mem_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "update_target_frequency = 100\n",
    "clip_error = False\n",
    "\n",
    "double_dqn = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    return egreedy_final + (egreedy - egreedy_final) * math.exp(-1 * steps_done / egreedy_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    " \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory[self.position] = transition\n",
    "        \n",
    "        self.position = ( self.position + 1 ) % self.capacity\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,hidden_layer)\n",
    "        self.advantage = nn.Linear(hidden_layer,hidden_layer)\n",
    "        self.advantage2 = nn.Linear(hidden_layer, number_of_outputs)\n",
    "        self.value = nn.Linear(hidden_layer,hidden_layer)\n",
    "        self.value2 = nn.Linear(hidden_layer,1)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        \n",
    "        output_advantage = self.advantage(output1)\n",
    "        output_advantage = self.activation(output_advantage)\n",
    "        output_advantage = self.advantage2(output_advantage)\n",
    "        \n",
    "        output_value = self.value(output1)\n",
    "        output_value = self.activation(output_value)\n",
    "        output_value = self.value2(output_value)\n",
    "        \n",
    "        return output_value + output_advantage - output_advantage.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetAgent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork()\n",
    "        self.target_nn = NeuralNetwork()\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        self.update_target_counter = 0\n",
    "        \n",
    "    def select_action(self, state, epsion):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = torch.Tensor(state)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn, 0)[1]\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = torch.Tensor(state) #.to(device)\n",
    "        new_state = torch.Tensor(new_state) #.to(device)\n",
    "        reward = torch.Tensor(reward) #.to(device)\n",
    "        action = torch.LongTensor(action) #.to(device)\n",
    "        done = torch.Tensor(done) #.to(device)\n",
    "        \n",
    "        if double_dqn:\n",
    "            new_state_indexes = self.nn(new_state).detach()\n",
    "            max_new_state_indexes = torch.max(new_state_indexes, 1)[1]  \n",
    "            \n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = new_state_values.gather(1, max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "            \n",
    "        target_value = reward + ( 1 - done ) * gamma * max_new_state_values\n",
    "  \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1,1)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        self.update_target_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 0 ***                       \n",
      "Av.reward: [last 10]: 2.00, [last 100]: 0.20, [all]: 20.00                       \n",
      "epsilon: 0.86, frames_total: 20\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 10 ***                       \n",
      "Av.reward: [last 10]: 32.50, [last 100]: 3.45, [all]: 31.36                       \n",
      "epsilon: 0.45, frames_total: 345\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 20 ***                       \n",
      "Av.reward: [last 10]: 70.30, [last 100]: 10.48, [all]: 49.90                       \n",
      "epsilon: 0.11, frames_total: 1048\n",
      "Elapsed time:  00:00:02\n",
      "\n",
      "*** Episode 30 ***                       \n",
      "Av.reward: [last 10]: 132.30, [last 100]: 23.71, [all]: 76.48                       \n",
      "epsilon: 0.01, frames_total: 2371\n",
      "Elapsed time:  00:00:05\n",
      "\n",
      "*** Episode 40 ***                       \n",
      "Av.reward: [last 10]: 155.50, [last 100]: 39.26, [all]: 95.76                       \n",
      "epsilon: 0.00, frames_total: 3926\n",
      "Elapsed time:  00:00:09\n",
      "\n",
      "*** Episode 50 ***                       \n",
      "Av.reward: [last 10]: 145.40, [last 100]: 53.80, [all]: 105.49                       \n",
      "epsilon: 0.00, frames_total: 5380\n",
      "Elapsed time:  00:00:13\n",
      "\n",
      "*** Episode 60 ***                       \n",
      "Av.reward: [last 10]: 154.90, [last 100]: 69.29, [all]: 113.59                       \n",
      "epsilon: 0.00, frames_total: 6929\n",
      "Elapsed time:  00:00:16\n",
      "\n",
      "*** Episode 70 ***                       \n",
      "Av.reward: [last 10]: 157.60, [last 100]: 85.05, [all]: 119.79                       \n",
      "epsilon: 0.00, frames_total: 8505\n",
      "Elapsed time:  00:00:20\n",
      "\n",
      "*** Episode 80 ***                       \n",
      "Av.reward: [last 10]: 164.40, [last 100]: 101.49, [all]: 125.30                       \n",
      "epsilon: 0.00, frames_total: 10149\n",
      "Elapsed time:  00:00:24\n",
      "\n",
      "*** Episode 90 ***                       \n",
      "Av.reward: [last 10]: 162.20, [last 100]: 117.71, [all]: 129.35                       \n",
      "epsilon: 0.00, frames_total: 11771\n",
      "Elapsed time:  00:00:28\n",
      "\n",
      "*** Episode 100 ***                       \n",
      "Av.reward: [last 10]: 163.40, [last 100]: 133.85, [all]: 132.72                       \n",
      "epsilon: 0.00, frames_total: 13405\n",
      "Elapsed time:  00:00:32\n",
      "\n",
      "*** Episode 110 ***                       \n",
      "Av.reward: [last 10]: 176.70, [last 100]: 148.27, [all]: 136.68                       \n",
      "epsilon: 0.00, frames_total: 15172\n",
      "Elapsed time:  00:00:37\n",
      "\n",
      "*** Episode 120 ***                       \n",
      "Av.reward: [last 10]: 192.50, [last 100]: 160.49, [all]: 141.30                       \n",
      "epsilon: 0.00, frames_total: 17097\n",
      "Elapsed time:  00:00:42\n",
      "\n",
      "*** Episode 130 ***                       \n",
      "Av.reward: [last 10]: 184.10, [last 100]: 165.67, [all]: 144.56                       \n",
      "epsilon: 0.00, frames_total: 18938\n",
      "Elapsed time:  00:00:46\n",
      "\n",
      "*** Episode 140 ***                       \n",
      "Av.reward: [last 10]: 186.30, [last 100]: 168.75, [all]: 147.52                       \n",
      "epsilon: 0.00, frames_total: 20801\n",
      "Elapsed time:  00:00:51\n",
      "\n",
      "*** Episode 150 ***                       \n",
      "Av.reward: [last 10]: 193.40, [last 100]: 173.55, [all]: 150.56                       \n",
      "epsilon: 0.00, frames_total: 22735\n",
      "Elapsed time:  00:00:55\n",
      "\n",
      "*** Episode 160 ***                       \n",
      "Av.reward: [last 10]: 199.20, [last 100]: 177.98, [all]: 153.58                       \n",
      "epsilon: 0.00, frames_total: 24727\n",
      "Elapsed time:  00:01:00\n",
      "\n",
      "*** Episode 170 ***                       \n",
      "Av.reward: [last 10]: 197.40, [last 100]: 181.96, [all]: 156.15                       \n",
      "epsilon: 0.00, frames_total: 26701\n",
      "Elapsed time:  00:01:06\n",
      "\n",
      "*** Episode 180 ***                       \n",
      "Av.reward: [last 10]: 197.60, [last 100]: 185.28, [all]: 158.44                       \n",
      "epsilon: 0.00, frames_total: 28677\n",
      "Elapsed time:  00:01:10\n",
      "\n",
      "*** Episode 190 ***                       \n",
      "Av.reward: [last 10]: 199.50, [last 100]: 189.01, [all]: 160.59                       \n",
      "epsilon: 0.00, frames_total: 30672\n",
      "Elapsed time:  00:01:15\n",
      "\n",
      "*** Episode 200 ***                       \n",
      "Av.reward: [last 10]: 186.40, [last 100]: 191.31, [all]: 161.87                       \n",
      "epsilon: 0.00, frames_total: 32536\n",
      "Elapsed time:  00:01:20\n",
      "\n",
      "*** Episode 210 ***                       \n",
      "Av.reward: [last 10]: 190.00, [last 100]: 192.64, [all]: 163.20                       \n",
      "epsilon: 0.00, frames_total: 34436\n",
      "Elapsed time:  00:01:25\n",
      "\n",
      "*** Episode 220 ***                       \n",
      "Av.reward: [last 10]: 196.50, [last 100]: 193.04, [all]: 164.71                       \n",
      "epsilon: 0.00, frames_total: 36401\n",
      "Elapsed time:  00:01:30\n",
      "\n",
      "*** Episode 230 ***                       \n",
      "Av.reward: [last 10]: 195.50, [last 100]: 194.18, [all]: 166.04                       \n",
      "epsilon: 0.00, frames_total: 38356\n",
      "Elapsed time:  00:01:35\n",
      "SOLVED! After 234 episodes \n",
      "\n",
      "*** Episode 240 ***                       \n",
      "Av.reward: [last 10]: 193.50, [last 100]: 194.90, [all]: 167.18                       \n",
      "epsilon: 0.00, frames_total: 40291\n",
      "Elapsed time:  00:01:41\n",
      "\n",
      "*** Episode 250 ***                       \n",
      "Av.reward: [last 10]: 197.10, [last 100]: 195.27, [all]: 168.37                       \n",
      "epsilon: 0.00, frames_total: 42262\n",
      "Elapsed time:  00:01:47\n",
      "\n",
      "*** Episode 260 ***                       \n",
      "Av.reward: [last 10]: 197.20, [last 100]: 195.07, [all]: 169.48                       \n",
      "epsilon: 0.00, frames_total: 44234\n",
      "Elapsed time:  00:01:52\n",
      "\n",
      "*** Episode 270 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 195.33, [all]: 170.61                       \n",
      "epsilon: 0.00, frames_total: 46234\n",
      "Elapsed time:  00:01:57\n",
      "\n",
      "*** Episode 280 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 195.57, [all]: 171.65                       \n",
      "epsilon: 0.00, frames_total: 48234\n",
      "Elapsed time:  00:02:02\n",
      "\n",
      "*** Episode 290 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 195.62, [all]: 172.63                       \n",
      "epsilon: 0.00, frames_total: 50234\n",
      "Elapsed time:  00:02:08\n",
      "\n",
      "*** Episode 300 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 196.98, [all]: 173.53                       \n",
      "epsilon: 0.00, frames_total: 52234\n",
      "Elapsed time:  00:02:14\n",
      "\n",
      "*** Episode 310 ***                       \n",
      "Av.reward: [last 10]: 174.90, [last 100]: 195.47, [all]: 173.58                       \n",
      "epsilon: 0.00, frames_total: 53983\n",
      "Elapsed time:  00:02:19\n",
      "\n",
      "*** Episode 320 ***                       \n",
      "Av.reward: [last 10]: 186.90, [last 100]: 194.51, [all]: 173.99                       \n",
      "epsilon: 0.00, frames_total: 55852\n",
      "Elapsed time:  00:02:23\n",
      "\n",
      "*** Episode 330 ***                       \n",
      "Av.reward: [last 10]: 193.90, [last 100]: 194.35, [all]: 174.60                       \n",
      "epsilon: 0.00, frames_total: 57791\n",
      "Elapsed time:  00:02:28\n",
      "\n",
      "*** Episode 340 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 195.00, [all]: 175.34                       \n",
      "epsilon: 0.00, frames_total: 59791\n",
      "Elapsed time:  00:02:33\n",
      "\n",
      "*** Episode 350 ***                       \n",
      "Av.reward: [last 10]: 164.50, [last 100]: 191.74, [all]: 175.03                       \n",
      "epsilon: 0.00, frames_total: 61436\n",
      "Elapsed time:  00:02:37\n",
      "\n",
      "*** Episode 360 ***                       \n",
      "Av.reward: [last 10]: 182.70, [last 100]: 190.29, [all]: 175.24                       \n",
      "epsilon: 0.00, frames_total: 63263\n",
      "Elapsed time:  00:02:42\n",
      "\n",
      "*** Episode 370 ***                       \n",
      "Av.reward: [last 10]: 91.60, [last 100]: 179.45, [all]: 172.99                       \n",
      "epsilon: 0.00, frames_total: 64179\n",
      "Elapsed time:  00:02:44\n",
      "\n",
      "*** Episode 380 ***                       \n",
      "Av.reward: [last 10]: 31.00, [last 100]: 162.55, [all]: 169.26                       \n",
      "epsilon: 0.00, frames_total: 64489\n",
      "Elapsed time:  00:02:45\n",
      "\n",
      "*** Episode 390 ***                       \n",
      "Av.reward: [last 10]: 14.90, [last 100]: 144.04, [all]: 165.31                       \n",
      "epsilon: 0.00, frames_total: 64638\n",
      "Elapsed time:  00:02:45\n",
      "\n",
      "*** Episode 400 ***                       \n",
      "Av.reward: [last 10]: 22.20, [last 100]: 126.26, [all]: 161.75                       \n",
      "epsilon: 0.00, frames_total: 64860\n",
      "Elapsed time:  00:02:46\n",
      "\n",
      "*** Episode 410 ***                       \n",
      "Av.reward: [last 10]: 135.00, [last 100]: 122.27, [all]: 161.09                       \n",
      "epsilon: 0.00, frames_total: 66210\n",
      "Elapsed time:  00:02:49\n",
      "\n",
      "*** Episode 420 ***                       \n",
      "Av.reward: [last 10]: 178.00, [last 100]: 121.38, [all]: 161.50                       \n",
      "epsilon: 0.00, frames_total: 67990\n",
      "Elapsed time:  00:02:53\n",
      "\n",
      "*** Episode 430 ***                       \n",
      "Av.reward: [last 10]: 168.60, [last 100]: 118.85, [all]: 161.66                       \n",
      "epsilon: 0.00, frames_total: 69676\n",
      "Elapsed time:  00:02:57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 440 ***                       \n",
      "Av.reward: [last 10]: 190.80, [last 100]: 117.93, [all]: 162.32                       \n",
      "epsilon: 0.00, frames_total: 71584\n",
      "Elapsed time:  00:03:02\n",
      "\n",
      "*** Episode 450 ***                       \n",
      "Av.reward: [last 10]: 155.10, [last 100]: 116.99, [all]: 162.16                       \n",
      "epsilon: 0.00, frames_total: 73135\n",
      "Elapsed time:  00:03:05\n",
      "\n",
      "*** Episode 460 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 118.72, [all]: 162.98                       \n",
      "epsilon: 0.00, frames_total: 75135\n",
      "Elapsed time:  00:03:10\n",
      "\n",
      "*** Episode 470 ***                       \n",
      "Av.reward: [last 10]: 181.50, [last 100]: 127.71, [all]: 163.38                       \n",
      "epsilon: 0.00, frames_total: 76950\n",
      "Elapsed time:  00:03:15\n",
      "\n",
      "*** Episode 480 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 144.61, [all]: 164.14                       \n",
      "epsilon: 0.00, frames_total: 78950\n",
      "Elapsed time:  00:03:19\n",
      "\n",
      "*** Episode 490 ***                       \n",
      "Av.reward: [last 10]: 188.10, [last 100]: 161.93, [all]: 164.63                       \n",
      "epsilon: 0.00, frames_total: 80831\n",
      "Elapsed time:  00:03:24\n",
      "Average reward: 165.26\n",
      "Average number of steps (last 100 episodes): 177.89\n",
      "Solved after 234 episodes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1188a3f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qnet_agent = QNetAgent()\n",
    "memory = ExperienceReplay(replay_mem_size)\n",
    "\n",
    "steps_total = []\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    step = 0\n",
    "    while True:\n",
    "        \n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        \n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        qnet_agent.optimize()\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            \n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            \n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "            \n",
    "            if (i_episode % report_interval == 0):\n",
    "                \n",
    "                \n",
    "                \n",
    "                print(\"\\n*** Episode %i *** \\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f \\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\" \n",
    "                  % \n",
    "                  ( i_episode,\n",
    "                    report_interval,\n",
    "                    sum(steps_total[-report_interval:])/report_interval,\n",
    "                    mean_reward_100,\n",
    "                    sum(steps_total)/len(steps_total),\n",
    "                    epsilon,\n",
    "                    frames_total\n",
    "                          ) \n",
    "                  )\n",
    "                  \n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time: \", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "print(\"Average reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "      print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color='red')\n",
    "plt.show()\n",
    "      \n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
