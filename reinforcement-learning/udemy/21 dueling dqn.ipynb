{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "\n",
    "import math, random, time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_episodes = 500\n",
    "gamma = 1\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "replay_mem_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "update_target_frequency = 100\n",
    "clip_error = False\n",
    "\n",
    "double_dqn = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    return egreedy_final + (egreedy - egreedy_final) * math.exp(-1 * steps_done / egreedy_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    " \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory[self.position] = transition\n",
    "        \n",
    "        self.position = ( self.position + 1 ) % self.capacity\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs, hidden_layer)\n",
    "        self.advantage = nn.Linear(hidden_layer, number_of_outputs)\n",
    "        self.value = nn.Linear(hidden_layer,1)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        \n",
    "        output_advantage = self.advantage(output1)\n",
    "        output_value = self.value(output1)\n",
    "        \n",
    "        return output_value + output_advantage - output_advantage.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetAgent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork()\n",
    "        self.target_nn = NeuralNetwork()\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        self.update_target_counter = 0\n",
    "        \n",
    "    def select_action(self, state, epsion):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = torch.Tensor(state)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn, 0)[1]\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = torch.Tensor(state) #.to(device)\n",
    "        new_state = torch.Tensor(new_state) #.to(device)\n",
    "        reward = torch.Tensor(reward) #.to(device)\n",
    "        action = torch.LongTensor(action) #.to(device)\n",
    "        done = torch.Tensor(done) #.to(device)\n",
    "        \n",
    "        if double_dqn:\n",
    "            new_state_indexes = self.nn(new_state).detach()\n",
    "            max_new_state_indexes = torch.max(new_state_indexes, 1)[1]  \n",
    "            \n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = new_state_values.gather(1, max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "            \n",
    "        target_value = reward + ( 1 - done ) * gamma * max_new_state_values\n",
    "  \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1,1)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        self.update_target_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 0 ***                       \n",
      "Av.reward: [last 10]: 1.20, [last 100]: 0.12, [all]: 12.00                       \n",
      "epsilon: 0.88, frames_total: 12\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 10 ***                       \n",
      "Av.reward: [last 10]: 14.50, [last 100]: 1.57, [all]: 14.27                       \n",
      "epsilon: 0.66, frames_total: 157\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 20 ***                       \n",
      "Av.reward: [last 10]: 16.90, [last 100]: 3.26, [all]: 15.52                       \n",
      "epsilon: 0.47, frames_total: 326\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 30 ***                       \n",
      "Av.reward: [last 10]: 40.60, [last 100]: 7.32, [all]: 23.61                       \n",
      "epsilon: 0.21, frames_total: 732\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 40 ***                       \n",
      "Av.reward: [last 10]: 143.20, [last 100]: 21.64, [all]: 52.78                       \n",
      "epsilon: 0.01, frames_total: 2164\n",
      "Elapsed time:  00:00:02\n",
      "\n",
      "*** Episode 50 ***                       \n",
      "Av.reward: [last 10]: 169.70, [last 100]: 38.61, [all]: 75.71                       \n",
      "epsilon: 0.00, frames_total: 3861\n",
      "Elapsed time:  00:00:05\n",
      "\n",
      "*** Episode 60 ***                       \n",
      "Av.reward: [last 10]: 175.00, [last 100]: 56.11, [all]: 91.98                       \n",
      "epsilon: 0.00, frames_total: 5611\n",
      "Elapsed time:  00:00:07\n",
      "\n",
      "*** Episode 70 ***                       \n",
      "Av.reward: [last 10]: 166.90, [last 100]: 72.80, [all]: 102.54                       \n",
      "epsilon: 0.00, frames_total: 7280\n",
      "Elapsed time:  00:00:09\n",
      "\n",
      "*** Episode 80 ***                       \n",
      "Av.reward: [last 10]: 165.00, [last 100]: 89.30, [all]: 110.25                       \n",
      "epsilon: 0.00, frames_total: 8930\n",
      "Elapsed time:  00:00:11\n",
      "\n",
      "*** Episode 90 ***                       \n",
      "Av.reward: [last 10]: 171.50, [last 100]: 106.45, [all]: 116.98                       \n",
      "epsilon: 0.00, frames_total: 10645\n",
      "Elapsed time:  00:00:13\n",
      "\n",
      "*** Episode 100 ***                       \n",
      "Av.reward: [last 10]: 171.50, [last 100]: 123.48, [all]: 122.38                       \n",
      "epsilon: 0.00, frames_total: 12360\n",
      "Elapsed time:  00:00:16\n",
      "\n",
      "*** Episode 110 ***                       \n",
      "Av.reward: [last 10]: 188.30, [last 100]: 140.86, [all]: 128.32                       \n",
      "epsilon: 0.00, frames_total: 14243\n",
      "Elapsed time:  00:00:18\n",
      "\n",
      "*** Episode 120 ***                       \n",
      "Av.reward: [last 10]: 194.30, [last 100]: 158.60, [all]: 133.77                       \n",
      "epsilon: 0.00, frames_total: 16186\n",
      "Elapsed time:  00:00:21\n",
      "\n",
      "*** Episode 130 ***                       \n",
      "Av.reward: [last 10]: 177.60, [last 100]: 172.30, [all]: 137.11                       \n",
      "epsilon: 0.00, frames_total: 17962\n",
      "Elapsed time:  00:00:23\n",
      "\n",
      "*** Episode 140 ***                       \n",
      "Av.reward: [last 10]: 186.40, [last 100]: 176.62, [all]: 140.61                       \n",
      "epsilon: 0.00, frames_total: 19826\n",
      "Elapsed time:  00:00:25\n",
      "\n",
      "*** Episode 150 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 179.65, [all]: 144.54                       \n",
      "epsilon: 0.00, frames_total: 21826\n",
      "Elapsed time:  00:00:28\n",
      "\n",
      "*** Episode 160 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 182.15, [all]: 147.99                       \n",
      "epsilon: 0.00, frames_total: 23826\n",
      "Elapsed time:  00:00:30\n",
      "\n",
      "*** Episode 170 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 185.46, [all]: 151.03                       \n",
      "epsilon: 0.00, frames_total: 25826\n",
      "Elapsed time:  00:00:33\n",
      "\n",
      "*** Episode 180 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 188.96, [all]: 153.73                       \n",
      "epsilon: 0.00, frames_total: 27826\n",
      "Elapsed time:  00:00:36\n",
      "\n",
      "*** Episode 190 ***                       \n",
      "Av.reward: [last 10]: 199.90, [last 100]: 191.80, [all]: 156.15                       \n",
      "epsilon: 0.00, frames_total: 29825\n",
      "Elapsed time:  00:00:38\n",
      "\n",
      "*** Episode 200 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 194.65, [all]: 158.33                       \n",
      "epsilon: 0.00, frames_total: 31825\n",
      "Elapsed time:  00:00:41\n",
      "SOLVED! After 201 episodes \n",
      "\n",
      "*** Episode 210 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 195.82, [all]: 160.31                       \n",
      "epsilon: 0.00, frames_total: 33825\n",
      "Elapsed time:  00:00:43\n",
      "\n",
      "*** Episode 220 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 196.39, [all]: 162.10                       \n",
      "epsilon: 0.00, frames_total: 35825\n",
      "Elapsed time:  00:00:46\n",
      "\n",
      "*** Episode 230 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 198.63, [all]: 163.74                       \n",
      "epsilon: 0.00, frames_total: 37825\n",
      "Elapsed time:  00:00:48\n",
      "\n",
      "*** Episode 240 ***                       \n",
      "Av.reward: [last 10]: 199.10, [last 100]: 199.90, [all]: 165.21                       \n",
      "epsilon: 0.00, frames_total: 39816\n",
      "Elapsed time:  00:00:51\n",
      "\n",
      "*** Episode 250 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.90, [all]: 166.60                       \n",
      "epsilon: 0.00, frames_total: 41816\n",
      "Elapsed time:  00:00:54\n",
      "\n",
      "*** Episode 260 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.90, [all]: 167.88                       \n",
      "epsilon: 0.00, frames_total: 43816\n",
      "Elapsed time:  00:00:56\n",
      "\n",
      "*** Episode 270 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.90, [all]: 169.06                       \n",
      "epsilon: 0.00, frames_total: 45816\n",
      "Elapsed time:  00:00:59\n",
      "\n",
      "*** Episode 280 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.90, [all]: 170.16                       \n",
      "epsilon: 0.00, frames_total: 47816\n",
      "Elapsed time:  00:01:01\n",
      "\n",
      "*** Episode 290 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.91, [all]: 171.19                       \n",
      "epsilon: 0.00, frames_total: 49816\n",
      "Elapsed time:  00:01:04\n",
      "\n",
      "*** Episode 300 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.91, [all]: 172.15                       \n",
      "epsilon: 0.00, frames_total: 51816\n",
      "Elapsed time:  00:01:06\n",
      "\n",
      "*** Episode 310 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.91, [all]: 173.04                       \n",
      "epsilon: 0.00, frames_total: 53816\n",
      "Elapsed time:  00:01:09\n",
      "\n",
      "*** Episode 320 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.91, [all]: 173.88                       \n",
      "epsilon: 0.00, frames_total: 55816\n",
      "Elapsed time:  00:01:12\n",
      "\n",
      "*** Episode 330 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.91, [all]: 174.67                       \n",
      "epsilon: 0.00, frames_total: 57816\n",
      "Elapsed time:  00:01:14\n",
      "\n",
      "*** Episode 340 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 175.41                       \n",
      "epsilon: 0.00, frames_total: 59816\n",
      "Elapsed time:  00:01:17\n",
      "\n",
      "*** Episode 350 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 176.11                       \n",
      "epsilon: 0.00, frames_total: 61816\n",
      "Elapsed time:  00:01:19\n",
      "\n",
      "*** Episode 360 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 176.78                       \n",
      "epsilon: 0.00, frames_total: 63816\n",
      "Elapsed time:  00:01:22\n",
      "\n",
      "*** Episode 370 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 177.40                       \n",
      "epsilon: 0.00, frames_total: 65816\n",
      "Elapsed time:  00:01:24\n",
      "\n",
      "*** Episode 380 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 177.99                       \n",
      "epsilon: 0.00, frames_total: 67816\n",
      "Elapsed time:  00:01:27\n",
      "\n",
      "*** Episode 390 ***                       \n",
      "Av.reward: [last 10]: 181.30, [last 100]: 198.13, [all]: 178.08                       \n",
      "epsilon: 0.00, frames_total: 69629\n",
      "Elapsed time:  00:01:29\n",
      "\n",
      "*** Episode 400 ***                       \n",
      "Av.reward: [last 10]: 189.10, [last 100]: 197.04, [all]: 178.35                       \n",
      "epsilon: 0.00, frames_total: 71520\n",
      "Elapsed time:  00:01:32\n",
      "\n",
      "*** Episode 410 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 197.04, [all]: 178.88                       \n",
      "epsilon: 0.00, frames_total: 73520\n",
      "Elapsed time:  00:01:34\n",
      "\n",
      "*** Episode 420 ***                       \n",
      "Av.reward: [last 10]: 195.50, [last 100]: 196.59, [all]: 179.28                       \n",
      "epsilon: 0.00, frames_total: 75475\n",
      "Elapsed time:  00:01:37\n",
      "\n",
      "*** Episode 430 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 196.59, [all]: 179.76                       \n",
      "epsilon: 0.00, frames_total: 77475\n",
      "Elapsed time:  00:01:39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 440 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 196.59, [all]: 180.22                       \n",
      "epsilon: 0.00, frames_total: 79475\n",
      "Elapsed time:  00:01:42\n",
      "\n",
      "*** Episode 450 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 196.59, [all]: 180.65                       \n",
      "epsilon: 0.00, frames_total: 81475\n",
      "Elapsed time:  00:01:44\n",
      "\n",
      "*** Episode 460 ***                       \n",
      "Av.reward: [last 10]: 191.60, [last 100]: 195.75, [all]: 180.89                       \n",
      "epsilon: 0.00, frames_total: 83391\n",
      "Elapsed time:  00:01:47\n",
      "\n",
      "*** Episode 470 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 195.75, [all]: 181.30                       \n",
      "epsilon: 0.00, frames_total: 85391\n",
      "Elapsed time:  00:01:49\n",
      "\n",
      "*** Episode 480 ***                       \n",
      "Av.reward: [last 10]: 189.00, [last 100]: 194.65, [all]: 181.46                       \n",
      "epsilon: 0.00, frames_total: 87281\n",
      "Elapsed time:  00:01:52\n",
      "\n",
      "*** Episode 490 ***                       \n",
      "Av.reward: [last 10]: 191.60, [last 100]: 195.68, [all]: 181.66                       \n",
      "epsilon: 0.00, frames_total: 89197\n",
      "Elapsed time:  00:01:54\n",
      "Average reward: 181.06\n",
      "Average number of steps (last 100 episodes): 192.12\n",
      "Solved after 201 episodes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117def710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qnet_agent = QNetAgent()\n",
    "memory = ExperienceReplay(replay_mem_size)\n",
    "\n",
    "steps_total = []\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    step = 0\n",
    "    while True:\n",
    "        \n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        \n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        qnet_agent.optimize()\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            \n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            \n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "            \n",
    "            if (i_episode % report_interval == 0):\n",
    "                \n",
    "                \n",
    "                \n",
    "                print(\"\\n*** Episode %i *** \\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f \\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\" \n",
    "                  % \n",
    "                  ( i_episode,\n",
    "                    report_interval,\n",
    "                    sum(steps_total[-report_interval:])/report_interval,\n",
    "                    mean_reward_100,\n",
    "                    sum(steps_total)/len(steps_total),\n",
    "                    epsilon,\n",
    "                    frames_total\n",
    "                          ) \n",
    "                  )\n",
    "                  \n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time: \", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "print(\"Average reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "      print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color='red')\n",
    "plt.show()\n",
    "      \n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
