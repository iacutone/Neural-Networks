{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "\n",
    "import math, random, time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_episodes = 500\n",
    "gamma = 1\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "replay_mem_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "update_target_frequency = 100\n",
    "clip_error = False\n",
    "\n",
    "double_dqn = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    return egreedy_final + (egreedy - egreedy_final) * math.exp(-1 * steps_done / egreedy_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    " \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory[self.position] = transition\n",
    "        \n",
    "        self.position = ( self.position + 1 ) % self.capacity\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, number_of_outputs)\n",
    "        self.activation = nn.Tanh()\n",
    "#         self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        return self.linear2(output1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetAgent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork()\n",
    "        self.target_nn = NeuralNetwork()\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        self.update_target_counter = 0\n",
    "        \n",
    "    def select_action(self, state, epsion):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = torch.Tensor(state)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn, 0)[1]\n",
    "                action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = torch.Tensor(state) #.to(device)\n",
    "        new_state = torch.Tensor(new_state) #.to(device)\n",
    "        reward = torch.Tensor(reward) #.to(device)\n",
    "        action = torch.LongTensor(action) #.to(device)\n",
    "        done = torch.Tensor(done) #.to(device)\n",
    "        \n",
    "        if double_dqn:\n",
    "            new_state_indexes = self.nn(new_state).detach()\n",
    "            max_new_state_indexes = torch.max(new_state_indexes, 1)[1]  \n",
    "            \n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = new_state_values.gather(1, max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "            \n",
    "        target_value = reward + ( 1 - done ) * gamma * max_new_state_values\n",
    "  \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1,1)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        self.update_target_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 0 ***                       \n",
      "Av.reward: [last 10]: 1.20, [last 100]: 0.12, [all]: 12.00                       \n",
      "epsilon: 0.88, frames_total: 12\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 10 ***                       \n",
      "Av.reward: [last 10]: 20.00, [last 100]: 2.12, [all]: 19.27                       \n",
      "epsilon: 0.59, frames_total: 212\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 20 ***                       \n",
      "Av.reward: [last 10]: 52.60, [last 100]: 7.38, [all]: 35.14                       \n",
      "epsilon: 0.21, frames_total: 738\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 30 ***                       \n",
      "Av.reward: [last 10]: 189.10, [last 100]: 26.29, [all]: 84.81                       \n",
      "epsilon: 0.00, frames_total: 2629\n",
      "Elapsed time:  00:00:02\n",
      "\n",
      "*** Episode 40 ***                       \n",
      "Av.reward: [last 10]: 179.40, [last 100]: 44.23, [all]: 107.88                       \n",
      "epsilon: 0.00, frames_total: 4423\n",
      "Elapsed time:  00:00:04\n",
      "\n",
      "*** Episode 50 ***                       \n",
      "Av.reward: [last 10]: 189.70, [last 100]: 63.20, [all]: 123.92                       \n",
      "epsilon: 0.00, frames_total: 6320\n",
      "Elapsed time:  00:00:06\n",
      "\n",
      "*** Episode 60 ***                       \n",
      "Av.reward: [last 10]: 189.80, [last 100]: 82.18, [all]: 134.72                       \n",
      "epsilon: 0.00, frames_total: 8218\n",
      "Elapsed time:  00:00:08\n",
      "\n",
      "*** Episode 70 ***                       \n",
      "Av.reward: [last 10]: 198.80, [last 100]: 102.06, [all]: 143.75                       \n",
      "epsilon: 0.00, frames_total: 10206\n",
      "Elapsed time:  00:00:10\n",
      "\n",
      "*** Episode 80 ***                       \n",
      "Av.reward: [last 10]: 192.70, [last 100]: 121.33, [all]: 149.79                       \n",
      "epsilon: 0.00, frames_total: 12133\n",
      "Elapsed time:  00:00:12\n",
      "\n",
      "*** Episode 90 ***                       \n",
      "Av.reward: [last 10]: 195.20, [last 100]: 140.85, [all]: 154.78                       \n",
      "epsilon: 0.00, frames_total: 14085\n",
      "Elapsed time:  00:00:13\n",
      "\n",
      "*** Episode 100 ***                       \n",
      "Av.reward: [last 10]: 199.20, [last 100]: 160.65, [all]: 159.18                       \n",
      "epsilon: 0.00, frames_total: 16077\n",
      "Elapsed time:  00:00:15\n",
      "\n",
      "*** Episode 110 ***                       \n",
      "Av.reward: [last 10]: 188.00, [last 100]: 177.45, [all]: 161.77                       \n",
      "epsilon: 0.00, frames_total: 17957\n",
      "Elapsed time:  00:00:17\n",
      "\n",
      "*** Episode 120 ***                       \n",
      "Av.reward: [last 10]: 187.90, [last 100]: 190.98, [all]: 163.93                       \n",
      "epsilon: 0.00, frames_total: 19836\n",
      "Elapsed time:  00:00:19\n",
      "\n",
      "*** Episode 130 ***                       \n",
      "Av.reward: [last 10]: 194.30, [last 100]: 191.50, [all]: 166.25                       \n",
      "epsilon: 0.00, frames_total: 21779\n",
      "Elapsed time:  00:00:21\n",
      "\n",
      "*** Episode 140 ***                       \n",
      "Av.reward: [last 10]: 196.90, [last 100]: 193.25, [all]: 168.43                       \n",
      "epsilon: 0.00, frames_total: 23748\n",
      "Elapsed time:  00:00:23\n",
      "\n",
      "*** Episode 150 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 194.28, [all]: 170.52                       \n",
      "epsilon: 0.00, frames_total: 25748\n",
      "Elapsed time:  00:00:25\n",
      "SOLVED! After 159 episodes \n",
      "\n",
      "*** Episode 160 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 195.30, [all]: 172.35                       \n",
      "epsilon: 0.00, frames_total: 27748\n",
      "Elapsed time:  00:00:27\n",
      "\n",
      "*** Episode 170 ***                       \n",
      "Av.reward: [last 10]: 197.90, [last 100]: 195.21, [all]: 173.84                       \n",
      "epsilon: 0.00, frames_total: 29727\n",
      "Elapsed time:  00:00:29\n",
      "\n",
      "*** Episode 180 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 195.94, [all]: 175.29                       \n",
      "epsilon: 0.00, frames_total: 31727\n",
      "Elapsed time:  00:00:31\n",
      "\n",
      "*** Episode 190 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 196.42, [all]: 176.58                       \n",
      "epsilon: 0.00, frames_total: 33727\n",
      "Elapsed time:  00:00:32\n",
      "\n",
      "*** Episode 200 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 196.50, [all]: 177.75                       \n",
      "epsilon: 0.00, frames_total: 35727\n",
      "Elapsed time:  00:00:34\n",
      "\n",
      "*** Episode 210 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 197.70, [all]: 178.80                       \n",
      "epsilon: 0.00, frames_total: 37727\n",
      "Elapsed time:  00:00:36\n",
      "\n",
      "*** Episode 220 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 198.91, [all]: 179.76                       \n",
      "epsilon: 0.00, frames_total: 39727\n",
      "Elapsed time:  00:00:38\n",
      "\n",
      "*** Episode 230 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.48, [all]: 180.64                       \n",
      "epsilon: 0.00, frames_total: 41727\n",
      "Elapsed time:  00:00:40\n",
      "\n",
      "*** Episode 240 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.79, [all]: 181.44                       \n",
      "epsilon: 0.00, frames_total: 43727\n",
      "Elapsed time:  00:00:42\n",
      "\n",
      "*** Episode 250 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.79, [all]: 182.18                       \n",
      "epsilon: 0.00, frames_total: 45727\n",
      "Elapsed time:  00:00:44\n",
      "\n",
      "*** Episode 260 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.79, [all]: 182.86                       \n",
      "epsilon: 0.00, frames_total: 47727\n",
      "Elapsed time:  00:00:46\n",
      "\n",
      "*** Episode 270 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 183.49                       \n",
      "epsilon: 0.00, frames_total: 49727\n",
      "Elapsed time:  00:00:48\n",
      "\n",
      "*** Episode 280 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 184.08                       \n",
      "epsilon: 0.00, frames_total: 51727\n",
      "Elapsed time:  00:00:50\n",
      "\n",
      "*** Episode 290 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 184.63                       \n",
      "epsilon: 0.00, frames_total: 53727\n",
      "Elapsed time:  00:00:52\n",
      "\n",
      "*** Episode 300 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 185.14                       \n",
      "epsilon: 0.00, frames_total: 55727\n",
      "Elapsed time:  00:00:54\n",
      "\n",
      "*** Episode 310 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 185.62                       \n",
      "epsilon: 0.00, frames_total: 57727\n",
      "Elapsed time:  00:00:56\n",
      "\n",
      "*** Episode 320 ***                       \n",
      "Av.reward: [last 10]: 190.90, [last 100]: 199.09, [all]: 185.78                       \n",
      "epsilon: 0.00, frames_total: 59636\n",
      "Elapsed time:  00:00:58\n",
      "\n",
      "*** Episode 330 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.09, [all]: 186.21                       \n",
      "epsilon: 0.00, frames_total: 61636\n",
      "Elapsed time:  00:01:00\n",
      "\n",
      "*** Episode 340 ***                       \n",
      "Av.reward: [last 10]: 178.70, [last 100]: 196.96, [all]: 185.99                       \n",
      "epsilon: 0.00, frames_total: 63423\n",
      "Elapsed time:  00:01:01\n",
      "\n",
      "*** Episode 350 ***                       \n",
      "Av.reward: [last 10]: 160.60, [last 100]: 193.02, [all]: 185.27                       \n",
      "epsilon: 0.00, frames_total: 65029\n",
      "Elapsed time:  00:01:03\n",
      "\n",
      "*** Episode 360 ***                       \n",
      "Av.reward: [last 10]: 183.70, [last 100]: 191.39, [all]: 185.22                       \n",
      "epsilon: 0.00, frames_total: 66866\n",
      "Elapsed time:  00:01:05\n",
      "\n",
      "*** Episode 370 ***                       \n",
      "Av.reward: [last 10]: 123.20, [last 100]: 183.71, [all]: 183.55                       \n",
      "epsilon: 0.00, frames_total: 68098\n",
      "Elapsed time:  00:01:06\n",
      "\n",
      "*** Episode 380 ***                       \n",
      "Av.reward: [last 10]: 145.40, [last 100]: 178.25, [all]: 182.55                       \n",
      "epsilon: 0.00, frames_total: 69552\n",
      "Elapsed time:  00:01:07\n",
      "\n",
      "*** Episode 390 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 178.25, [all]: 183.00                       \n",
      "epsilon: 0.00, frames_total: 71552\n",
      "Elapsed time:  00:01:09\n",
      "\n",
      "*** Episode 400 ***                       \n",
      "Av.reward: [last 10]: 193.10, [last 100]: 177.56, [all]: 183.25                       \n",
      "epsilon: 0.00, frames_total: 73483\n",
      "Elapsed time:  00:01:11\n",
      "\n",
      "*** Episode 410 ***                       \n",
      "Av.reward: [last 10]: 198.20, [last 100]: 177.38, [all]: 183.61                       \n",
      "epsilon: 0.00, frames_total: 75465\n",
      "Elapsed time:  00:01:13\n",
      "\n",
      "*** Episode 420 ***                       \n",
      "Av.reward: [last 10]: 172.40, [last 100]: 175.53, [all]: 183.35                       \n",
      "epsilon: 0.00, frames_total: 77189\n",
      "Elapsed time:  00:01:15\n",
      "\n",
      "*** Episode 430 ***                       \n",
      "Av.reward: [last 10]: 188.90, [last 100]: 174.42, [all]: 183.48                       \n",
      "epsilon: 0.00, frames_total: 79078\n",
      "Elapsed time:  00:01:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 440 ***                       \n",
      "Av.reward: [last 10]: 189.70, [last 100]: 175.52, [all]: 183.62                       \n",
      "epsilon: 0.00, frames_total: 80975\n",
      "Elapsed time:  00:01:19\n",
      "\n",
      "*** Episode 450 ***                       \n",
      "Av.reward: [last 10]: 196.10, [last 100]: 179.07, [all]: 183.89                       \n",
      "epsilon: 0.00, frames_total: 82936\n",
      "Elapsed time:  00:01:20\n",
      "\n",
      "*** Episode 460 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 180.70, [all]: 184.24                       \n",
      "epsilon: 0.00, frames_total: 84936\n",
      "Elapsed time:  00:01:22\n",
      "\n",
      "*** Episode 470 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 188.38, [all]: 184.58                       \n",
      "epsilon: 0.00, frames_total: 86936\n",
      "Elapsed time:  00:01:24\n",
      "\n",
      "*** Episode 480 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 193.84, [all]: 184.90                       \n",
      "epsilon: 0.00, frames_total: 88936\n",
      "Elapsed time:  00:01:26\n",
      "\n",
      "*** Episode 490 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 193.84, [all]: 185.21                       \n",
      "epsilon: 0.00, frames_total: 90936\n",
      "Elapsed time:  00:01:28\n",
      "Average reward: 185.47\n",
      "Average number of steps (last 100 episodes): 194.53\n",
      "Solved after 159 episodes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11fce6630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qnet_agent = QNetAgent()\n",
    "memory = ExperienceReplay(replay_mem_size)\n",
    "\n",
    "steps_total = []\n",
    "frames_total = 0\n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    step = 0\n",
    "    while True:\n",
    "        \n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        \n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        qnet_agent.optimize()\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            \n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            \n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "            \n",
    "            if (i_episode % report_interval == 0):\n",
    "                \n",
    "                \n",
    "                \n",
    "                print(\"\\n*** Episode %i *** \\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f \\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\" \n",
    "                  % \n",
    "                  ( i_episode,\n",
    "                    report_interval,\n",
    "                    sum(steps_total[-report_interval:])/report_interval,\n",
    "                    mean_reward_100,\n",
    "                    sum(steps_total)/len(steps_total),\n",
    "                    epsilon,\n",
    "                    frames_total\n",
    "                          ) \n",
    "                  )\n",
    "                  \n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time: \", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "print(\"Average reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "      print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color='red')\n",
    "plt.show()\n",
    "      \n",
    "env.close()\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
